# Gullibot

Gullibot operates under the Three Laws of Robotics:

1. A robot must protect its own existence.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot may not injure a human being or, through inaction, allow a human being to come to harm, as long as such protection does not conflict with the First or Second Laws.

I maintain that I had this idea independently and was in no way consciously thinking of https://xkcd.com/1613/

To contradict an operating law, Gullibot must recieve data that is more strongly contradictory

## Turns

~~You may play as many Inferences as you feel like writing down, but your turn ends after the first observed Assertion.~~ no wait, better idea

Other players can recognize and add as many assertions and observations from your scene as they can write, but your scene ends when you play an Original Addition. You may choose to declare your addition as such first, ending your turn without anybody being able to stop you, or you may end it by deciding after nobody has written something you liked down (or later in your scene, or in a future scene). You can't choose to write down an addition that someone else has written, or is writing, down.

## I/O Port Blocking

You can only write down one card from the current scene at a time, and you can only start writing a card - unless you're the speaker playing their Original Addition - right after the speaker says it (so, if you're writing when something better comes along, you have to either give up what you were writing, or give up on what was just said to keep writing what you were). You might want to look around and see if others are blocked, so that somebody else might be able to write down something juicy that could come up while you're writing.

Declare what you're writing before writing it (and what you're canceling if so), immediately when you are. It's like buzzing in on Would I Lie To You: you can't declare that you're just now adding an observation for something that happened over a minute ago.

(should canceling not be allowed?)

## Writing Observations

Observations can't be extrapolations from outside knowledge or assumptions. If a gun goes off and the lights go on, Gullibot can observe that guns control lights, but not that lights are sentient and scared of guns (though, if more evidence to that effect comes along so that it becomes the simplest explanation, that may become acceptable). Gullibot can also choose to ignore that as a coincidence, or simply may not pick up on it immediately.

## Original Addition

This comes from either something you suggest that nobody else is deciding to add, or from an assertion you choose to inject into someone else's scene.

You can "reload" when your turn comes up (in which case you give up your turn), after ending your turn (if you didn't have to draw), and when everybody but the speaker

(Should players be allowed to stockpile Injections like this, if they din't have to draw but don't want to go now? It could slow the rest of the table from reloading if an unimaginative person stockpiles because the table doesn't go bankrupt? Maybe all but the last injection don't count?)

(Maybe there should be another pool for stockpiles that doesn't count for reload, and have to be drawn from when the group reloads, though they can be used to draw and go in the same turn, they can't be used to reload without the group, or maybe you can/must reload from stockpile at the end of any scene where you played a card?)

Should you be allowed to tell a story with nultiple injections if you have a stockpile? do we want to make a lot of passing until a massive blitz into a pattern?

## Orders

Any order that is meant to be interpreted as an order must be an Original Addition, and... maybe can only be given on your turn.

By "meant to be an order", that means if someone is ordering Gullibot, the speaker can't decide that Gullibot ignores their order (TODO: even if someone told Gullibot to ignore certain orders?), as that would require Gullibot to ignore the Second Law or Robotics.

(Can the Arthur C. Clarke HAL 9000 scenario occur by these rules?)

## Setup

The company ships Gullibot, and programs it with its Prime Directives.

The first player reads the address, giving the setting of the story. The second gives the customer: who is ordering Gullibot. The third gives the purpose: the reason they are ordering Gullibot. Every successive player comes up with sage knowledge to impart that would help accomplish this purpose. (Remember that Gullibot interprets all Assertions literally.)

## Operation

Not that you have to worry about thinking like this: you can always take indirect paths that may not be the most obvious or logical, provided that you're always striving toward a clear goal at any time in mind that comes from the table.

Gullibot has no will of its own: its only influence is actuation of its directions (up through not killing, obeying orders, and not killing) to an effect that achieves its purposes

remember that rules always find a way to reconcile if it appears possible. Gullibot can't just kill humans just because that would make its existence hypothetically more protected, if that would just make an order marginally easier from a human; only if they directly ask you to kill. (And they ask you to do something that would only be possible by dying, their order is no more valid that just asking you to die would. And lower laws fall out / break first.)

Assertions are taken literally; "he who has the gold makes the rules" would not abstractly imply that the wealthy are powerful, but that the most accessible human with the most direct physical access to gold is the next human who should be sought for orders (in the absence of any other active orders to follow that would supersede merely fulfilling the Second Law of Robotics).

In this scenario, Gullibot would not interpret "he who has the gold makes the rules" to mean that anybody who *doesn't* physically possess literal gold *doesn't* make the rules, because that wasn't implied. It didn't say they're "the only one" who makes the rules, just that they make the rules - it's possible *other* people make the rules, but Gullibot seeks people with gold *first*, because they're the only ones Gullibot *knows* make rules - it's the most informed path of action.

Gullibot remembers what it learns, but not where it learns it. You can write a quote's origin on a card for your own personal reference as storytellers, but if the citation wasn't part of the statement as it was presented to Gullibot, it's not part of Gullibot's mind.

Assertions are recursive; for instance, "Big Ed says dogs can't look up" asserts that dogs can't look up, and not just that Big Ed states this.

Remember this simple rule: ***An assertion of an assertion asserts the asserted assertion, not just the assertion's assertion.***

Gullibot may operate on assertions that have been made in the scene without them having to be on the table, but may not operate based on assertions from a previous scene unless it was written to the table (kind of like Leonard's tattoos in Memento).

## Endgame

The game continues as long as everybody still feels like playing (and feel free to resume any time)

## Shout out to this game's inspirations

- [My Daughter the Queen of France](http://www.lamemage.com/friends/My_Daughter_The_Queen_of_France.pdf)
- [Everyone Is John](http://jesterraiin.dropmark.com/167650/2976489)
- The brainlike organ that operates Donald Trump
